\documentclass[10pt,a4paper]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{ {./image/} }
\begin{document}
\begin{titlepage}
\begin{center}
\Large\textbf{Report DLP}\\
\large\textit{Bao,Viet}
\end{center}
\end{titlepage}
\section{Text2SQL}
\subsection{problem}
This task is to produce sql queries based on inputs consits of natural language question and database. The task can be formalized as: \\\\
Given a natural language $Q$ and the schema $S = \langle \mathcal{T},\mathcal{C}\rangle$ for a relational database, the parser needs to generate the corresponding SQL query $Y$. The schema consists of tables $\mathcal{T} = \{t_1,..,t_N\}$ and fields $\mathcal{C} =\{c_{11},...,c_{1|T_1|},...,c_{n1},...,c_{N|T_N|} \}$.Each table $t_i$ and each field $c_{ij}$ has a textual name. Some fields are primary keys, used for uniquely indexing each data record, and some are foreign keys, used to reference a primary key in a different table. In addition, each field has a data type, $\tau\in\{number,text,time,boolean,etc\}$.\\\\
 In the paper Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing, there are many databases, and users can input abitrary question, the job of the model is to generate stuible query based on the database it's given.
\begin{figure}[htbp] 
\begin{center}
\includegraphics[scale=0.3]{./image/2.png}
\end{center}
\end{figure}  
\section{Literature review}
Text2SQL recently the field has witnessed a re-surge of interest for text-to-SQL semantic parsing, by virtue of newly released large-scale datasets and matured neural network modeling tools. The task uses the model consists of 2 parts: encoder and decoder. Some works like  (Guo et al., 2019; Wang et al.,
2019; Choi et al., 2020; Furrer et al., 2020). Bogin
et al. (2019a,b) encode schemas as graphs and use graph strcutures to guide decoding.  Guo et al. (2019) proposes schema-linking and SemQL, an intermediate SQL representation customized for questions in the Spider dataset which was synthesized via a tree-based decode.. Wang et al. (2019) proposes RAT-SQL, a unified graph encoding mechanism which effec-tively covers relations in the schema graph and its linking with the question. The overall architecture of RAT-SQL is deep, consisting of 8 relational self-attention layers (Shaw et al., 2018) on top of BERT-large.
\begin{figure}[htbp] 
\begin{center}
\includegraphics[scale=0.3]{./image/3.png}
\end{center}
\end{figure}
\\ 
Text2SQL can be formulized as seq2seq problem by concatenating natural question and sequential database, then decoder generates sql form. Shaw et al. (2020) shows that the T5 model (Raffel
et al., 2020) with 3 billion parameters achieves the
state-of-the-art performance on Spider. \\
Text2SQL can be enchaned by using DB content. Shaw et al. (2019) shows
that value information is critical to the cross-DB
semantic parsing tasks. \\
BRIDGE is a general framework for jointly representing question, DB schema
and the relevant DB cells. BRIDGE serialized the relational DB schema and uses BERT to model cross-
table dependencies. It uses anchor texts which provide more focused signals that link the text and the DB schema.
\section{Methods}
BRIDGE epresent each table with
its table name followed by its fields. Each table
name is preceded by the special token $[T]$ and each
field name is preceded by $[C]$. The representations
of multiple tables are concatenated to form a se-
rialization of the schema, which is surrounded by
two $[SEP]$ tokens and concatenated to the question.
Finally, following the input format of BERT, the
question is preceded by $[CLS]$ to form the hybrid
question-schema serialization.
\begin{center}
$X = [CLS],Q,[SEP],[T],t_{1},[C],c_{11},...,c_{1|T_1|},[T],t_{2},[C],c_{21},...,c_{N|T_N|},[SEP]$
\end{center}
X is encoded by a Bi-LSTM layer, the question segments output is then passed to another Bi-LSTM. Each table/field is represented
using the slice of output X corresponding to its special
token $[T]/[C]$.\\
Primary keys, foreign keys and data types are also trained by using dense lookup features. These feature then are fused with output X to form final encoding of schema.\\
$\textbf{h}^{t_{i}}_{S} = g([\textbf{h}^{p}_{X};\textbf{0};\textbf{0};\textbf{0}])$ \\
$\textbf{h}^{c_{ij}}_{S} = g([\textbf{h}^{q}_{X};f^{u}_{pri},f^{v}_{for},f^{w}_{type}])$  $=ReLU(\textbf{W}_{g}[\textbf{h}^{q}_{X};f^{u}_{pri},f^{v}_{for},f^{w}_{type}]+\textbf{b}_{g})$\\
$\textbf{h}_{S} = [\textbf{h}^{t_{1}},...,\textbf{h}^{t_{|\tau|}},\textbf{h}^{c_{11}},...,\textbf{h}^{c_{N|T_{N}|}}]$\\\\
Use of anchor text
to link value mentions in the question with the cor-
responding DB fields by performing fuzzy string
match between $Q$ and the picklist of each field in
the DB. The matched field values (anchor texts)
are inserted into the question-schema representa-
tion $X$, succeeding the corresponding field names
and separated by the special token $[V]$. If multiple
values were matched for one field, we concatenate
all of them in matching order. If a ques-
tion mention is matched with values in multiple
fields. We add all matches and let the model learn
to resolve ambiguity. \\ 
The decoder is initiated with the final state of the question encoder. At each step, the decoder performs one of the following actions: generating a token from the vocabulary $V$, copying a token from the question $Q$ or copying a schema component from $S$.\\
\begin{center}
$e_{tj}^{(h)} = \frac{s_{t}W^{(h)}_{U}(h_{j}W^{(h)}_{V})^\top}{\sqrt{n/H}};\space \alpha_{tj}^{(h)}=\underset{j}{softmax}\{e_{tj}^{(h)}\}$\\ 
$z_{t}^{(h)} = \sum^{|Q|+|S|}_{j=1}\alpha_{tj}^{(h)}(h_{j}W^{(h)}_{V});z_{t}=[z_{t}^{(1)},...,z_{t}^{(H)}]$\\
$p^{t}_{gen}=sigmoid(s_{t}W^{s}_{gen}+z_{t}W^{z}_{gen}+b_{gen})$\\
$p^{t}_{out} = p^{t}_{gen}P_{\mathcal{V}}(y_{t})+(1-p^{t}_{gen})\sum_{j: \overset{\sim}{X}_{j}=y_{t}}\alpha_{tj}^{(H)})$
\end{center}
\begin{figure}[htbp] 
\begin{center}
\includegraphics[scale=0.3]{./image/1.png}
\end{center}
\end{figure}
The fact that the DB fields ap-
peared in each SQL clause must only come from
the tables in the FROM clause. \\
\textbf{Lemma 1} Let $Y_{exec}$ be a SQL query with clauses
arranged in execution order, then any table field in
$Y_{exec}$ must appear after the table.\\
As a result, we adopt a binary attention mask $\xi$\\
$\overset{\sim}{\alpha}_{t}^{(H)}=\alpha_{t}^{(H)} . \xi$\\
which initially has entries corresponding to all
fields set to 0 Once a table $t_{i}$ is decoded, 
all entries in $\xi$ corresponding to that table to 1, allows the decoder to only search in the space
specified by the condition in Lemma 1 with little
overhead in decoding speed.
\section{Experiment result}
Train/dev/test databases from spider dataset not overlap, test set is hidden from public
In WikiSQL, 49.6\% of its dev tables and 45.1\% of its test tables are not found in the train set. Both databases have the ability of models to generalize to unseen schema in train set.\\\\
\textbf{Exact Match (EM)}: Checks if the predicted SQL exactly matches the ground truth SQL.\\
\textbf{Exact Set Match (E-SM)}: Structural correctness of the predicted SQL. Check orderless set match.\\
\textbf{Execution Accuracy (EA)}: Execution results have the same result.\\\\
\textbf{Anchor Text selection}: Fuzzy matching \emph{k} from DB field picklist and exclude number due to not effectively discriminate between different fields. \\
Training use cross-entropy Loss and Adam-SGD optimizer with mini-batch size of 32 and
for model, it uses BERT-large + 1-layer LSTMs + 8-head attention

\includegraphics[scale=1.5]{non-numeric}

\begin{tabular}{ |l|c|c| }
\hline
  Model & Dev & Test \\
\hline
  Global-GNN (Bogin et al., 2019b) & 52.7 & 47.4 \\
  EditSQL + BERT (Zhang et al., 2019) & 57.6 & 53.4 \\
  GNN + Bertrand-DR (Kelkar et al., 2020) & 57.9 & 54.6 \\
  IRNet + BERT (Guo et al., 2019) & 61.9 & 54.7 \\
  RAT-SQL v2 (Wang et al., 2019) & 62.7 & 57.2 \\
  RYANSQL + BERTL (Choi et al., 2020) & 66.6 & 58.2 \\
  SmBoP + BART (Rubin and Berant, 2020) & 66.0 & 60.5 \\
  RYANSQL v2 + BERTL & 70.6 & 60.6 \\
  RAT-SQL v3 + BERTL (Wang et al., 2019) & 69.7 & 65.6 \\
  BRIDGE v1 (Lin et al., 2020) & 65.5 & 59.2 \\
  BRIDGE L (ours) & 70.0 & 65.0 \\
  BRIDGE L (ours, ensemble) & 71.1 & 67.5 \\
\hline
\end{tabular}
\section{Error analysis}
50 ramdom Spider dev set failed samples. There are 4 common errors: Logical, Lexical Understanding, Commonsense, Robustness.\\\\
\includegraphics[scale=1.2]{error}\\
\includegraphics[scale=1.2]{error2}
\section{References}
1. https://github.com/salesforce/TabularSemanticParsing \\
2. arXiv:1706.03762 \\
3. Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888, Online. Association for Computational Linguistics.\\
4. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7567–7578, Online. Association for Computational Linguistics.\\
5. https://www.microsoft.com/en-us/research/uploads/prod/2018/07/Execution-Guided-Neural-Program-Decoding.pdf
\end{document}